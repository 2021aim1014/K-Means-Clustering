{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNViZzwTUtTmauNV5HVERt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2021aim1014/K-Means-Clustering/blob/main/K_Means_Clustering_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from numpy.random import uniform\n",
        "from sklearn.datasets import make_blobs\n",
        "import seaborn as sns\n",
        "import random"
      ],
      "metadata": {
        "id": "mOpByxnIRHqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean(point, data):\n",
        "    return np.sqrt(np.sum((point - data)**2, axis=1))"
      ],
      "metadata": {
        "id": "qL2FQ5fARJ33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KMeans:\n",
        "    def __init__(self, n_clusters=8, max_iter=300):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self, X_train):\n",
        "        # Initialize the centroids, using the \"k-means++\" method, where a random datapoint is selected as the first,\n",
        "        # then the rest are initialized w/ probabilities proportional to their distances to the first\n",
        "        # Pick a random point from train data for first centroid\n",
        "        self.centroids = [random.choice(X_train)]\n",
        "        for _ in range(self.n_clusters-1):\n",
        "            # Calculate distances from points to the centroids\n",
        "            dists = np.sum([euclidean(centroid, X_train) for centroid in self.centroids], axis=0)\n",
        "            # Normalize the distances\n",
        "            dists /= np.sum(dists)\n",
        "            # Choose remaining points based on their distances\n",
        "            new_centroid_idx, = np.random.choice(range(len(X_train)), size=1, p=dists)\n",
        "            self.centroids += [X_train[new_centroid_idx]]\n",
        "        # This initial method of randomly selecting centroid starts is less effective\n",
        "        # min_, max_ = np.min(X_train, axis=0), np.max(X_train, axis=0)\n",
        "        # self.centroids = [uniform(min_, max_) for _ in range(self.n_clusters)]\n",
        "        # Iterate, adjusting centroids until converged or until passed max_iter\n",
        "        iteration = 0\n",
        "        prev_centroids = None\n",
        "\n",
        "        while np.not_equal(self.centroids, prev_centroids).any() and iteration < self.max_iter:\n",
        "            # Sort each datapoint, assigning to nearest centroid\n",
        "            sorted_points = [[] for _ in range(self.n_clusters)]\n",
        "            for x in X_train:\n",
        "                dists = euclidean(x, self.centroids)\n",
        "                centroid_idx = np.argmin(dists)\n",
        "                sorted_points[centroid_idx].append(x)\n",
        "            # Push current centroids to previous, reassign centroids as mean of the points belonging to them\n",
        "            prev_centroids = self.centroids\n",
        "            self.centroids = [np.mean(cluster, axis=0) for cluster in sorted_points]\n",
        "            for i, centroid in enumerate(self.centroids):\n",
        "                if np.isnan(centroid).any():  # Catch any np.nans, resulting from a centroid having no points\n",
        "                    self.centroids[i] = prev_centroids[i]\n",
        "            iteration += 1\n",
        "            print(iteration)\n",
        "            self.inertia_ = np.sum([ np.min(dist)**2 for dist in self.transform(X_train)])\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transform to cluster-distance space\n",
        "        # X: pd.DataFrame, independent variables, float\n",
        "        # return dists = list of [dist to centroid 1, dist to centroid 2, ...]\n",
        "        dists = [[self.dist(x,centroid) for centroid in self.centroids] for x in X]\n",
        "        return dists\n",
        "\n",
        "    def dist(self, point, data):\n",
        "        return np.sqrt(np.sum((point - data)**2))\n",
        "        \n",
        "    def evaluate(self, X):\n",
        "        centroids = []\n",
        "        centroid_idxs = []\n",
        "        for x in X:\n",
        "            dists = euclidean(x, self.centroids)\n",
        "            centroid_idx = np.argmin(dists)\n",
        "            centroids.append(self.centroids[centroid_idx])\n",
        "            centroid_idxs.append(centroid_idx)\n",
        "        return centroids, centroid_idxs"
      ],
      "metadata": {
        "id": "yXUyd9jBJKfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "BQ9cLHLPVL7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = (x_train[:5000], y_train[:5000]), (x_test[:5000], y_test[:5000])"
      ],
      "metadata": {
        "id": "o50eRmTFbvSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPz2HQRAWYG3",
        "outputId": "4309d2bc-f8eb-48e0-81f5-bd0c31a4040b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 28, 28)\n",
            "(5000,)\n",
            "(5000, 28, 28)\n",
            "(5000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (X_train, Y_train), (X_test, Y_test) = (x_train[:100], y_train[:100]), (x_test[:100], y_test[:100])"
      ],
      "metadata": {
        "id": "-6BJmd-uV0jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X_train.shape)\n",
        "# print(Y_train.shape)\n",
        "# print(X_test.shape)\n",
        "# print(Y_test.shape)"
      ],
      "metadata": {
        "id": "zuTIhSnqWk5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# defining feature extractor that we want to use\n",
        "extractor = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "def compute_features(image):\n",
        "    keypoints, descriptors = extractor.detectAndCompute(image, None)\n",
        "    return keypoints, descriptors"
      ],
      "metadata": {
        "id": "iIU5KP6pVNtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "rgAI2tAeSUF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CreateVisualDictionary(dataset):\n",
        "  descriptor_list = []\n",
        "  features = {}\n",
        "  for idx, image in tqdm(enumerate(dataset)):\n",
        "    k, d = compute_features(image)\n",
        "\n",
        "    if d is not None:\n",
        "      descriptor_list.extend(d)\n",
        "      features[idx] = d\n",
        "  return features, descriptor_list"
      ],
      "metadata": {
        "id": "6_OA_9PzVSfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_f, train_d = CreateVisualDictionary(x_train)\n",
        "f_test, d_test = CreateVisualDictionary(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbMtZQGnVUAa",
        "outputId": "2fce0596-6ab1-4674-8420-eeb97aa44765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5000it [00:03, 1261.05it/s]\n",
            "5000it [00:05, 868.03it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.cluster import KMeans\n",
        "def MatchHistogram(h1, h2):\n",
        "    return np.sqrt(np.sum((h1 - h2)**2, axis=1))"
      ],
      "metadata": {
        "id": "0LC6bjchVWC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=2)"
      ],
      "metadata": {
        "id": "n3KWqDTdVg-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.fit(train_d)"
      ],
      "metadata": {
        "id": "V8P8BLouVpXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.inertia_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWXVAZB9yvSc",
        "outputId": "08cbacb9-0dbc-4e80-d074-e1ebeef42287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2445575836.74039"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow = kmeans.centroids"
      ],
      "metadata": {
        "id": "jPbx1-7PVlmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vP4Ur-7VysQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add a function to save the sift of the image which is closest\n",
        "def ComputeHistogram(features, bow):\n",
        "  histograms = {}\n",
        "  for img in tqdm(features):\n",
        "    # all descriptors of the img\n",
        "    descr = features[img]\n",
        "    histogram = np.zeros(len(bow))\n",
        "    class_centers, classification = kmeans.evaluate(descr)\n",
        "    # for each des, find the cluster and create histogram\n",
        "    for p in classification:\n",
        "      p = int(p)\n",
        "      histogram[p] += 1\n",
        "    # update global histograms\n",
        "    histograms[img] = histogram\n",
        "  return histograms"
      ],
      "metadata": {
        "id": "AxbSfHv8W0Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_hist = ComputeHistogram(train_f, bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiDyBR26W9D1",
        "outputId": "681280d6-bc56-42b5-ffb6-b24f23c1aa1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4601/4601 [00:01<00:00, 2542.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(histograms, y_train):\n",
        "  trainX = []\n",
        "  trainY = []\n",
        "\n",
        "  for x in histograms:\n",
        "    trainX.append(histograms[x])\n",
        "    trainY.append(y_train[x])\n",
        "  return trainX, trainY"
      ],
      "metadata": {
        "id": "yyoRtpV6XZac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import LinearSVC"
      ],
      "metadata": {
        "id": "U2IbuD5HY4ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX, trainY = prepare_dataset(train_hist, y_train)"
      ],
      "metadata": {
        "id": "gkdF-RuxY57h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LinearSVC()\n",
        "clf.fit(trainX, trainY)\n",
        "preds = clf.predict(trainX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xJliHrOY7ai",
        "outputId": "80613afd-5748-4c50-a877-cf9120871a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_hist = ComputeHistogram(f_test, bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX2ryh3sY9ov",
        "outputId": "8a724b09-9a48-4213-868d-6a80b732f7cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4641/4641 [00:01<00:00, 2385.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testX, testY = prepare_dataset(test_hist, y_test)"
      ],
      "metadata": {
        "id": "yYjFfYC5ZIyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_test = clf.predict(testX)\n",
        "print(classification_report(testY, preds_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBPIG1DyZKmz",
        "outputId": "21318220-65fc-4493-f04d-a24c2e2acc6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.55      0.58       480\n",
            "           1       0.60      0.72      0.65       316\n",
            "           2       0.41      0.46      0.43       496\n",
            "           3       0.46      0.44      0.45       468\n",
            "           4       0.41      0.46      0.43       506\n",
            "           5       0.68      0.72      0.70       467\n",
            "           6       0.28      0.14      0.19       445\n",
            "           7       0.68      0.72      0.70       481\n",
            "           8       0.62      0.64      0.63       507\n",
            "           9       0.78      0.82      0.80       475\n",
            "\n",
            "    accuracy                           0.56      4641\n",
            "   macro avg       0.55      0.57      0.56      4641\n",
            "weighted avg       0.55      0.56      0.55      4641\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testY = np.array(testY)"
      ],
      "metadata": {
        "id": "0s056CV3csBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_test = np.array(preds_test)"
      ],
      "metadata": {
        "id": "Ra8KTiyFc1sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(testY == preds_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWZV1rruc7XG",
        "outputId": "4c7d8f22-13ba-42c3-9ca5-a4b140006fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5636716224951519"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FyxehV-ldGCb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}